{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:51:18.854104Z",
     "start_time": "2025-05-04T16:51:18.848447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import hamming_loss, f1_score, precision_score\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ],
   "id": "96ea0e39247c80a0",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:51:18.882013Z",
     "start_time": "2025-05-04T16:51:18.860864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "file_path = './dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Checking for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"Missing values in each column:\\n{missing_values}\")\n",
    "\n",
    "# Handling missing values (e.g., filling with the mean for numerical columns)\n",
    "# df.fillna(df.mean(), inplace=True)"
   ],
   "id": "2be0f29cbec2c73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "report                     0\n",
      "type_blocker               0\n",
      "type_regression            0\n",
      "type_bug                   0\n",
      "type_documentation         0\n",
      "type_enhancement           0\n",
      "type_task                  0\n",
      "type_dependency_upgrade    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:51:18.944221Z",
     "start_time": "2025-05-04T16:51:18.902812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, digits, and extra spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply text cleaning to the 'report' column\n",
    "df['cleaned_report'] = df['report'].apply(clean_text)"
   ],
   "id": "38591c4b3b49a7bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sefas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:51:19.053461Z",
     "start_time": "2025-05-04T16:51:18.964077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vectorize text using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(df['cleaned_report'])\n",
    "\n",
    "# Prepare the labels (binary matrix)\n",
    "labels = df.drop(columns=['report', 'cleaned_report'])\n",
    "y = labels.values\n",
    "\n",
    "# Remove 'type_task' label\n",
    "df = df.drop(columns=['type_task'])\n",
    "labels = df.drop(columns=['report', 'cleaned_report'])\n",
    "y = labels.values\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "4c9ee9f37b03bb5d",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:51:19.079683Z",
     "start_time": "2025-05-04T16:51:19.075954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "#\n",
    "# # Standardizing the TF-IDF features (feature scaling)\n",
    "# scaler = StandardScaler(with_mean=False)  # 'with_mean=False' because sparse matrices can't be centered\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n"
   ],
   "id": "c7c833b8e1b1d30c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:51:23.689388Z",
     "start_time": "2025-05-04T16:51:19.100552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train and evaluate different models using MultiOutputClassifier for multi-label classification\n",
    "\n",
    "# 1. Logistic Regression (One-vs-Rest)\n",
    "lr_model = LogisticRegression(solver='liblinear')\n",
    "lr_multi_model = MultiOutputClassifier(lr_model, n_jobs=-1)\n",
    "lr_multi_model.fit(X_train, y_train)\n",
    "lr_pred = lr_multi_model.predict(X_test)"
   ],
   "id": "87d0ecc5595a7581",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:51:26.267913Z",
     "start_time": "2025-05-04T16:51:23.717339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Support Vector Machine (SVM)\n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "svm_multi_model = MultiOutputClassifier(svm_model, n_jobs=-1)\n",
    "svm_multi_model.fit(X_train, y_train)\n",
    "svm_pred = svm_multi_model.predict(X_test)"
   ],
   "id": "9416bbc9d2d159e4",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:52:04.198068Z",
     "start_time": "2025-05-04T16:51:26.295305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Perceptron\n",
    "perceptron_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "perceptron_multi_model = MultiOutputClassifier(perceptron_model, n_jobs=-1)\n",
    "perceptron_multi_model.fit(X_train, y_train)\n",
    "perceptron_pred = perceptron_multi_model.predict(X_test)"
   ],
   "id": "3bf0420853987e9",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:52:45.952013Z",
     "start_time": "2025-05-04T16:52:04.218657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Deep Neural Network (DNN)\n",
    "dnn_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)\n",
    "dnn_multi_model = MultiOutputClassifier(dnn_model, n_jobs=-1)\n",
    "dnn_multi_model.fit(X_train, y_train)\n",
    "dnn_pred = dnn_multi_model.predict(X_test)"
   ],
   "id": "fafb2beb03062854",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:52:46.098684Z",
     "start_time": "2025-05-04T16:52:46.000925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate using multi-label metrics\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    # Hamming Loss\n",
    "    hamming = hamming_loss(y_true, y_pred)\n",
    "    # F1 Score (Micro)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    # F1 Score (Macro)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    # Precision (Micro)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    return hamming, f1_micro, f1_macro, precision_micro\n",
    "\n",
    "# Logistic Regression Evaluation\n",
    "lr_hamming, lr_f1_micro, lr_f1_macro, lr_precision = evaluate_model(y_test, lr_pred)\n",
    "print(f\"Logistic Regression - Hamming Loss: {lr_hamming}, F1 Micro: {lr_f1_micro}, F1 Macro: {lr_f1_macro}, Precision: {lr_precision}\")\n",
    "\n",
    "# SVM Evaluation\n",
    "svm_hamming, svm_f1_micro, svm_f1_macro, svm_precision = evaluate_model(y_test, svm_pred)\n",
    "print(f\"SVM - Hamming Loss: {svm_hamming}, F1 Micro: {svm_f1_micro}, F1 Macro: {svm_f1_macro}, Precision: {svm_precision}\")\n",
    "\n",
    "# Perceptron Evaluation\n",
    "perceptron_hamming, perceptron_f1_micro, perceptron_f1_macro, perceptron_precision = evaluate_model(y_test, perceptron_pred)\n",
    "print(f\"Perceptron - Hamming Loss: {perceptron_hamming}, F1 Micro: {perceptron_f1_micro}, F1 Macro: {perceptron_f1_macro}, Precision: {perceptron_precision}\")\n",
    "\n",
    "# DNN Evaluation\n",
    "dnn_hamming, dnn_f1_micro, dnn_f1_macro, dnn_precision = evaluate_model(y_test, dnn_pred)\n",
    "print(f\"DNN - Hamming Loss: {dnn_hamming}, F1 Micro: {dnn_f1_micro}, F1 Macro: {dnn_f1_macro}, Precision: {dnn_precision}\")\n"
   ],
   "id": "f49ae64baffc88ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Hamming Loss: 0.12170263788968826, F1 Micro: 0.8111627906976744, F1 Macro: 0.44579475953594544, Precision: 0.8433268858800773\n",
      "SVM - Hamming Loss: 0.11270983213429256, F1 Micro: 0.8278388278388278, F1 Macro: 0.608248484018128, Precision: 0.846441947565543\n",
      "Perceptron - Hamming Loss: 0.12170263788968826, F1 Micro: 0.8139321723189734, F1 Macro: 0.6227471048575731, Precision: 0.8330206378986866\n",
      "DNN - Hamming Loss: 0.12470023980815348, F1 Micro: 0.8077634011090573, F1 Macro: 0.5924611469639677, Precision: 0.833969465648855\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:52:46.173364Z",
     "start_time": "2025-05-04T16:52:46.141483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=3):\n",
    "    # Calculate Precision@k for multi-label classification\n",
    "    precision_at_k = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        top_k_preds = np.argsort(pred)[::-1][:k]  # Get indices of top-k predictions\n",
    "        correct_preds = np.sum([true[i] for i in top_k_preds])  # Check how many are correct\n",
    "        precision_at_k.append(correct_preds / k)  # Precision@k = correct / k\n",
    "    return np.mean(precision_at_k)\n",
    "\n",
    "# Add Precision@k to the evaluation\n",
    "precision_at_k_lr = precision_at_k(y_test, lr_pred)\n",
    "print(f\"Logistic Regression - Precision@k: {precision_at_k_lr}\")\n",
    "\n",
    "precision_at_k_svm = precision_at_k(y_test, svm_pred)\n",
    "print(f\"SVM - Precision@k: {precision_at_k_svm}\")\n"
   ],
   "id": "ea71ed7f173558ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Precision@k: 0.5815347721822542\n",
      "SVM - Precision@k: 0.5959232613908872\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
